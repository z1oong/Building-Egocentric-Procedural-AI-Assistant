# Building-Egocentric-Procedural-AI-Assistant

## Taxonomy

### Egocentric procedural error detection

#### Method

|Year|Venue|Paper Title|Link|
|:-:|:-:|-|-|
|2025|CVPR|[Modeling Multiple Normal Action Representations for Error Detection in Procedural Tasks](https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Modeling_Multiple_Normal_Action_Representations_for_Error_Detection_in_Procedural_CVPR_2025_paper.pdf) |[Project Page](https://github.com/iSEE-Laboratory/AMNAR)|
|2025|CVPR|[Gazing Into Missteps: Leveraging Eye-Gaze for Unsupervised Mistake Detection in Egocentric Videos of Skilled Human Activities](https://openaccess.thecvf.com/content/CVPR2025/papers/Mazzamuto_Gazing_Into_Missteps_Leveraging_Eye-Gaze_for_Unsupervised_Mistake_Detection_in_CVPR_2025_paper.pdf) |-|
|2025|arXiv|[Technical Report for Egocentric Mistake Detection for the HoloAssist Challenge](https://arxiv.org/pdf/2506.06174) |[Project Page](https://www.codabench.org/competitions/2613/)|
|2025|EMNLP|[Transparent and Coherent Procedural Mistake Detection](https://aclanthology.org/2025.emnlp-main.706.pdf)|-|
|2024|CVPR|[Error Detection in Egocentric Procedural Task Videos](https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Error_Detection_in_Egocentric_Procedural_Task_Videos_CVPR_2024_paper.pdf)|[Project Page](https://github.com/robert80203/EgoPER_official)|
|2024|CVPR|[PREGO: online mistake detection in PRocedural EGOcentric videos](https://openaccess.thecvf.com/content/CVPR2024/papers/Flaborea_PREGO_Online_Mistake_Detection_in_PRocedural_EGOcentric_Videos_CVPR_2024_paper.pdf)|[Project Page](https://github.com/aleflabo/PREGO)|
|2024|arXiv|[TI-PREGO: Chain of Thought and In-Context Learning for Online Mistake Detection in PRocedural EGOcentric Videos](https://arxiv.org/pdf/2411.02570)|-|
|2024|ICCV|[EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos referring to Procedural Texts](https://openaccess.thecvf.com/content/ICCV2025W/SAUAFG/papers/Haneji_EgoOops_A_Dataset_for_Mistake_Action_Detection_from_Egocentric_Videos_ICCVW_2025_paper.pdf)|[Project Page](https://y-haneji.github.io/EgoOops-project-page/)|
|2023|arXiv|[Every Mistake Counts in Assembly](https://arxiv.org/pdf/2307.16453)|-|

#### Datasets
|Year|Venue|Paper Title|Link|
|:-:|:-:|-|-|
|2024|NeurIPS|[CaptainCook4D: A Dataset for Understanding Errors in Procedural Activities](https://proceedings.neurips.cc/paper_files/paper/2024/file/f4a04396c2ed1342a5d8d05e94cb6101-Paper-Datasets_and_Benchmarks_Track.pdf)|[Project Page](https://captaincook4d.github.io/captain-cook/)|
|2024|WACV|[ndustreal: A dataset for procedure step recognition handling execution errors in egocentric videos in an industrial-like setting](https://openaccess.thecvf.com/content/WACV2024/papers/Schoonbeek_IndustReal_A_Dataset_for_Procedure_Step_Recognition_Handling_Execution_Errors_WACV_2024_paper.pdf)|[Project Page](https://github.com/TimSchoonbeek/IndustReal)|
|2024|CVPR|[Error Detection in Egocentric Procedural Task Videos](https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Error_Detection_in_Egocentric_Procedural_Task_Videos_CVPR_2024_paper.pdf)|[Project Page](https://github.com/robert80203/EgoPER_official)|
|2024|ICCV|[EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos referring to Procedural Texts](https://openaccess.thecvf.com/content/ICCV2025W/SAUAFG/papers/Haneji_EgoOops_A_Dataset_for_Mistake_Action_Detection_from_Egocentric_Videos_ICCVW_2025_paper.pdf)|[Project Page](https://y-haneji.github.io/EgoOops-project-page/)|
|2024|NeurIPS|[IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos](https://proceedings.neurips.cc/paper_files/paper/2024/file/936c72edff325b2ad5eda320e916487f-Paper-Datasets_and_Benchmarks_Track.pdf)|[Project Page](https://yunongliu1.github.io/ikea-video-manual/)|
|2024|CVPR|[Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives](https://openaccess.thecvf.com/content/CVPR2024/papers/Grauman_Ego-Exo4D_Understanding_Skilled_Human_Activity_from_First-_and_Third-Person_Perspectives_CVPR_2024_paper.pdf)|-|
|2023|ICCV|[HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.pdf)|[Project Page]( https://holoassist.github.io/)|
|2022|CVPR|[Ego4D: Around the World in 3,000 Hours of Egocentric Video](https://openaccess.thecvf.com/content/CVPR2022/papers/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.pdf)|[Project Page]( https://ego4d-data.org/)|
|2022|CVPR|[Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities](https://openaccess.thecvf.com/content/CVPR2022/papers/Sener_Assembly101_A_Large-Scale_Multi-View_Video_Dataset_for_Understanding_Procedural_Activities_CVPR_2022_paper.pdf)|[Project Page](https://assembly101.github.io/)|
|2020|TPAMI|[EgoCom: A Multi-Person Multi-Modal Egocentric Communications Dataset](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200754)|[Project Page](https://github.com/facebookresearch/EgoCom-Dataset)|
|2020|TPAMI|[The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9084270)|[Project Page]( http://epic-kitchens.github.io.)|
|2019|ICCV|[EPIC-Tent: An Egocentric Video Dataset for Camping Tent Assembly](https://openaccess.thecvf.com/content_ICCVW_2019/papers/EPIC/Jang_EPIC-Tent_An_Egocentric_Video_Dataset_for_Camping_Tent_Assembly_ICCVW_2019_paper.pdf)|-|

### Egocentric procedural learning

#### Method
|Year|Venue|Paper Title|Link|
|:-:|:-:|-|-|
|2024|WACV|[United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos](https://openaccess.thecvf.com/content/WACV2024/papers/Bansal_United_We_Stand_Divided_We_Fall_UnityGraph_for_Unsupervised_Procedure_WACV_2024_paper.pdf) |-|
|2019|ICCV|[Unsupervised Procedure Learning via Joint Dynamic Summarization](https://openaccess.thecvf.com/content_ICCV_2019/papers/Elhamifar_Unsupervised_Procedure_Learning_via_Joint_Dynamic_Summarization_ICCV_2019_paper.pdf)|-|
|2016|CVPR|[Unsupervised Learning from Narrated Instruction Videos](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Alayrac_Unsupervised_Learning_From_CVPR_2016_paper.pdf)|-|
|2015|ICCV|[Unsupervised Semantic Parsing of Video Collections](https://openaccess.thecvf.com/content_iccv_2015/papers/Sener_Unsupervised_Semantic_Parsing_ICCV_2015_paper.pdf)|-|

|2019|CVPR|[Unsupervised learning of action classes with continuous temporal embedding](https://openaccess.thecvf.com/content_CVPR_2019/papers/Kukleva_Unsupervised_Learning_of_Action_Classes_With_Continuous_Temporal_Embedding_CVPR_2019_paper.pdf)|[Project Page](https://github.com/annusha/unsup_temp_embed)|



